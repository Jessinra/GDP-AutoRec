{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing AutoRec\n",
    "\n",
    "Original paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Library.Autorec.data_preprocessor import *\n",
    "# from Library.Autorec.AutoRec import AutoRec\n",
    "\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from ipywidgets import FloatProgress, IntProgress\n",
    "from IPython.display import display\n",
    "\n",
    "current_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def mask_if_not_zero(matrix):\n",
    "    \n",
    "    nonzero_idx = matrix.nonzero()\n",
    "    keep = np.arange(len(nonzero_idx[0]))\n",
    "    n_keep = len(keep)\n",
    "\n",
    "    mask_csr = csr_matrix((np.ones(n_keep), (nonzero_idx[0][keep], nonzero_idx[1][keep])), shape=matrix.shape)\n",
    "    return mask_csr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import lil_matrix, csr_matrix\n",
    "\n",
    "def train_test_split(data, train_ratio, random_state=55):\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    # Initialize\n",
    "    train = data.copy()\n",
    "    test = lil_matrix(data.shape)\n",
    "    test_users_idx = set()\n",
    "    test_items_idx = set()\n",
    "    \n",
    "    # Usable rating for train / test\n",
    "    nonzero = data.nonzero()\n",
    "    n_nonzero = len(nonzero[0])\n",
    "    \n",
    "    n_train = int(train_ratio * n_nonzero)\n",
    "    n_test = n_nonzero - n_train\n",
    "    \n",
    "    sampled_idx = np.random.choice(np.arange(n_nonzero), size=n_test, replace=False)\n",
    "    \n",
    "    # Create train set\n",
    "    train_users_idx = set(np.arange(data.shape[0]))\n",
    "    train_items_idx = set(np.arange(data.shape[1]))\n",
    "\n",
    "\n",
    "    for idx in tqdm(sampled_idx):\n",
    "\n",
    "        row = nonzero[0][idx]\n",
    "        col = nonzero[1][idx]\n",
    "        \n",
    "        # Modify matrix\n",
    "        test[row, col] = train[row, col]        \n",
    "        train[row, col] = 0 \n",
    "        \n",
    "        # Add into test set\n",
    "        test_users_idx.add(row)\n",
    "        test_items_idx.add(col)\n",
    "\n",
    "    return train, test, n_train, n_test, \\\n",
    "           train_users_idx, train_items_idx, \\\n",
    "           test_users_idx, test_items_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from scipy import sparse\n",
    "\n",
    "def read_rating(path, train_ratio):\n",
    "    \n",
    "    filename = path + \"/ratings.csr\"\n",
    "    R = pickle.load(open(filename, 'rb'))\n",
    "    \n",
    "    train_R, test_R, n_train_R, n_test_R, train_users_idx, train_items_idx, test_users_idx, test_items_idx = train_test_split(R, train_ratio=train_ratio, random_state=55)\n",
    "    \n",
    "    mask_R = mask_if_not_zero(R)\n",
    "    train_mask_R = mask_if_not_zero(train_R)\n",
    "    test_mask_R = mask_if_not_zero(test_R)\n",
    "\n",
    "    return  R, mask_R, train_R, train_mask_R, test_R, test_mask_R, n_train_R, n_test_R, train_users_idx, train_items_idx, test_users_idx, test_items_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.hidden_neuron = 1000\n",
    "        self.lambda_value = 1.0\n",
    "        \n",
    "        self.train_epoch = 200\n",
    "        self.batch_size = 2048\n",
    "        \n",
    "        self.optimizer_method = 'Adam' # 'Adam','RMSProp'\n",
    "        self.grad_clip = False\n",
    "        self.base_lr = 0.003\n",
    "        self.decay_epoch_step = 50\n",
    "        \n",
    "        self.random_seed = 1000\n",
    "        self.display_step = 1\n",
    "        self.save_step = 5\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"hidden_neuron : {}\\nlambda_value : {}\\ntrain_epoch : {}\\nbatch_size : {}\\noptimizer_method : {}\\ngrad_clip : {}\\nbase_lr : {}\\ndecay_epoch_step : {}\\nrandom_seed : {}\\ndisplay_step : {}\\n\".format(\n",
    "            self.hidden_neuron,\n",
    "            self.lambda_value,\n",
    "            self.train_epoch,\n",
    "            self.batch_size,\n",
    "            self.optimizer_method,\n",
    "            self.grad_clip,\n",
    "            self.base_lr,\n",
    "            self.decay_epoch_step,\n",
    "            self.random_seed,\n",
    "            self.display_step,\n",
    "            self.save_step)\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "class Logger:\n",
    "\n",
    "    def set_default_filename(self, filename):\n",
    "        self.default_filename = filename\n",
    "\n",
    "    def create_session_folder(self, path):\n",
    "        try:  \n",
    "            os.makedirs(path)\n",
    "        except OSError:  \n",
    "            print (\"Creation of the directory %s failed\" % path)\n",
    "        else:  \n",
    "            print (\"     ===> Successfully created the directory %s \\n\" % path)\n",
    "\n",
    "    def log(self, text):\n",
    "        with open(self.default_filename, 'a') as f:\n",
    "            f.writelines(text)\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "    def save_model(self, model, filename):\n",
    "        pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "from datetime import datetime\n",
    "from scipy.sparse import lil_matrix, csr_matrix, vstack\n",
    "from tqdm import tqdm\n",
    "\n",
    "class AutoRec():\n",
    "    def __init__(self, sess, args,\n",
    "                 num_users, num_items,\n",
    "                 R, mask_R, train_R, train_mask_R, test_R, test_mask_R, num_train_ratings, num_test_ratings,\n",
    "                 user_train_set, item_train_set, user_test_set, item_test_set):\n",
    "\n",
    "        self.sess = sess\n",
    "        self.args = args\n",
    "\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "\n",
    "        self.R = R\n",
    "        self.mask_R = mask_R\n",
    "        self.train_R = train_R\n",
    "        self.train_mask_R = train_mask_R\n",
    "        self.test_r = test_R\n",
    "        self.test_mask_R = test_mask_R\n",
    "        self.num_train_ratings = num_train_ratings\n",
    "        self.num_test_ratings = num_test_ratings\n",
    "\n",
    "        self.user_train_set = user_train_set\n",
    "        self.item_train_set = item_train_set\n",
    "        self.user_test_set = user_test_set\n",
    "        self.item_test_set = item_test_set\n",
    "\n",
    "        self.hidden_neuron = args.hidden_neuron\n",
    "        self.train_epoch = args.train_epoch\n",
    "        self.batch_size = args.batch_size\n",
    "        self.num_batch = int(\n",
    "            math.ceil(self.num_users / float(self.batch_size)))\n",
    "\n",
    "        self.base_lr = args.base_lr\n",
    "        self.optimizer_method = args.optimizer_method\n",
    "        self.display_step = args.display_step\n",
    "        self.random_seed = args.random_seed\n",
    "\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        self.decay_epoch_step = args.decay_epoch_step\n",
    "        self.decay_step = self.decay_epoch_step * self.num_batch\n",
    "        self.lr = tf.train.exponential_decay(self.base_lr, self.global_step,\n",
    "                                             self.decay_step, 0.96, staircase=True)\n",
    "        self.lambda_value = args.lambda_value\n",
    "\n",
    "        self.train_cost_list = []\n",
    "        self.test_cost_list = []\n",
    "        self.test_rmse_list = []\n",
    "\n",
    "        self.grad_clip = args.grad_clip\n",
    "        \n",
    "        self.timestamp = str(datetime.timestamp(datetime.now()))\n",
    "        self.logger = Logger()\n",
    "        self.session_log_path = \"../log/{}/\".format(self.timestamp)\n",
    "        self.logger.create_session_folder(self.session_log_path)\n",
    "        self.logger.set_default_filename(self.session_log_path + \"log.txt\")\n",
    "\n",
    "    def run(self):\n",
    "        \n",
    "        # Log parameters\n",
    "        self.logger.log(str(self.args))\n",
    "        self.prepare_model()\n",
    "        \n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess.run(init)\n",
    "        \n",
    "        for epoch_itr in (range(self.train_epoch)):\n",
    "            \n",
    "            self.train_model(epoch_itr)\n",
    "            self.test_model(epoch_itr)\n",
    "            \n",
    "            # Save the variables to disk.\n",
    "            if epoch_iter % self.save_step == 0:\n",
    "                self.saver.save(self.sess, self.session_log_path + \"models/epoch_{}\".format(epoch_itr))\n",
    "            \n",
    "        self.make_records()\n",
    "\n",
    "\n",
    "    def prepare_model(self):\n",
    "        self.input_R = tf.placeholder(dtype=tf.float32, shape=[\n",
    "                                      None, self.num_items], name=\"input_R\")\n",
    "        self.input_mask_R = tf.placeholder(\n",
    "            dtype=tf.float32, shape=[None, self.num_items], name=\"input_mask_R\")\n",
    "\n",
    "        V = tf.get_variable(name=\"V\", initializer=tf.truncated_normal(shape=[self.num_items, self.hidden_neuron],\n",
    "                                                                      mean=0, stddev=0.03), dtype=tf.float32)\n",
    "        W = tf.get_variable(name=\"W\", initializer=tf.truncated_normal(shape=[self.hidden_neuron, self.num_items],\n",
    "                                                                      mean=0, stddev=0.03), dtype=tf.float32)\n",
    "        mu = tf.get_variable(name=\"mu\", initializer=tf.zeros(\n",
    "            shape=self.hidden_neuron), dtype=tf.float32)\n",
    "        b = tf.get_variable(name=\"b\", initializer=tf.zeros(\n",
    "            shape=self.num_items), dtype=tf.float32)\n",
    "\n",
    "        pre_Encoder = tf.matmul(self.input_R, V) + mu\n",
    "        self.Encoder = tf.nn.sigmoid(pre_Encoder)\n",
    "        \n",
    "        pre_Decoder = tf.matmul(self.Encoder, W) + b\n",
    "        self.decoder = tf.identity(pre_Decoder)\n",
    "\n",
    "        pre_rec_cost = tf.multiply(\n",
    "            (self.input_R - self.decoder), self.input_mask_R)\n",
    "        rec_cost = tf.square(self.l2_norm(pre_rec_cost))\n",
    "        pre_reg_cost = tf.square(self.l2_norm(W)) + tf.square(self.l2_norm(V))\n",
    "        reg_cost = self.lambda_value * 0.5 * pre_reg_cost\n",
    "\n",
    "        self.cost = rec_cost + reg_cost\n",
    "\n",
    "        if self.optimizer_method == \"Adam\":\n",
    "            optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "        elif self.optimizer_method == \"RMSProp\":\n",
    "            optimizer = tf.train.RMSPropOptimizer(self.lr)\n",
    "        else:\n",
    "            raise ValueError(\"Optimizer Key ERROR\")\n",
    "\n",
    "        if self.grad_clip:\n",
    "            gvs = optimizer.compute_gradients(self.cost)\n",
    "            capped_gvs = [(tf.clip_by_value(grad, -5., 5.), var)\n",
    "                          for grad, var in gvs]\n",
    "            self.optimizer = optimizer.apply_gradients(\n",
    "                capped_gvs, global_step=self.global_step)\n",
    "        else:\n",
    "            self.optimizer = optimizer.minimize(\n",
    "                self.cost, global_step=self.global_step)\n",
    "            \n",
    "        self.saver = tf.train.Saver(max_to_keep=None)\n",
    "\n",
    "    def train_model(self, itr):\n",
    "        start_time = time.time()\n",
    "        random_perm_doc_idx = np.random.permutation(self.num_users)\n",
    "\n",
    "        batch_cost = 0\n",
    "        for i in tqdm(range(self.num_batch)):\n",
    "\n",
    "            if i >= self.num_batch - 1:\n",
    "                batch_set_idx = random_perm_doc_idx[i * self.batch_size:]\n",
    "            else:\n",
    "                batch_set_idx = random_perm_doc_idx[i * self.batch_size: (i + 1) * self.batch_size]\n",
    "\n",
    "            _, cost = self.sess.run(\n",
    "                [self.optimizer, self.cost],\n",
    "                feed_dict={self.input_R: self.train_R[batch_set_idx, :].todense(),\n",
    "                           self.input_mask_R: self.train_mask_R[batch_set_idx, :].todense()})\n",
    "\n",
    "            batch_cost = batch_cost + cost\n",
    "\n",
    "        self.train_cost_list.append(batch_cost)\n",
    "\n",
    "        if (itr + 1) % self.display_step == 0:\n",
    "            self.logger.log(\n",
    "                \"Training Epoch {}\\tTotal cost = {:.2f}\\tElapsed time : {} sec\".format(\n",
    "                    itr, batch_cost, (time.time() - start_time)))\n",
    "            \n",
    "            print(\n",
    "                \"===== Training =====\\n\"\n",
    "                \"Epoch {} \\t Total cost = {:.2f}\\n\"\n",
    "                \"Elapsed time : {} sec\\n\".format(\n",
    "                    itr, batch_cost, (time.time() - start_time)))\n",
    "\n",
    "    def test_model(self, itr):\n",
    "        start_time = time.time()\n",
    "\n",
    "        batch_cost = 0\n",
    "        numerator = 0\n",
    "        \n",
    "        for i in tqdm(range(self.num_batch)):\n",
    "\n",
    "            # Batching idx\n",
    "            batch_start_idx = i * self.batch_size\n",
    "            if i >= self.num_batch - 1:\n",
    "                batch_stop_idx = batch_start_idx + \\\n",
    "                                 (self.num_users - 1) % self.batch_size + 1\n",
    "            else:\n",
    "                batch_stop_idx = (i + 1) * self.batch_size\n",
    "\n",
    "            cost, decoder = self.sess.run(\n",
    "                [self.cost, self.decoder],\n",
    "                feed_dict={self.input_R: self.test_r[batch_start_idx:batch_stop_idx].todense(),\n",
    "                           self.input_mask_R: self.test_mask_R[batch_start_idx:batch_stop_idx].todense()})\n",
    "            \n",
    "            batch_cost += cost\n",
    "\n",
    "            # Make prediction if need to show\n",
    "            if (itr + 1) % self.display_step == 0:\n",
    "\n",
    "                batch_predict_r = csr_matrix(decoder.clip(min=0.2, max=1))\n",
    "\n",
    "                # Some statistic\n",
    "                predicted_rating_delta = batch_predict_r - self.test_r[batch_start_idx:batch_stop_idx]\n",
    "                pre_numerator = self.test_mask_R[batch_start_idx:batch_stop_idx].multiply(predicted_rating_delta)\n",
    "                numerator += np.sum(pre_numerator.data ** 2)\n",
    "\n",
    "        self.test_cost_list.append(batch_cost)\n",
    "\n",
    "        # Make prediction if need to show\n",
    "        if (itr + 1) % self.display_step == 0:\n",
    "\n",
    "            denominator = self.num_test_ratings\n",
    "            RMSE = np.sqrt(numerator / float(denominator))\n",
    "            self.test_rmse_list.append(RMSE)\n",
    "\n",
    "            self.logger.log(\n",
    "                \"Testing Epoch {}\\tTotal cost = {:.2f}\\tRMSE = {:.5f}\\tElapsed time : {} sec\".format(\n",
    "                    itr, batch_cost, RMSE, (time.time() - start_time)))\n",
    "\n",
    "            print(\n",
    "                \"===== Testing =====\\n\"\n",
    "                \"Epoch {} \\t Total cost = {:.2f}\\n\"\n",
    "                \"RMSE = {:.5f} \\t Elapsed time : {} sec\\n\".format(\n",
    "                    itr, batch_cost, RMSE, (time.time() - start_time)))\n",
    "\n",
    "    def make_records(self):\n",
    "\n",
    "        basic_info = self.session_log_path + \"basic_info.txt\"\n",
    "        train_record = self.session_log_path + \"train_record.txt\"\n",
    "        test_record = self.session_log_path + \"test_record.txt\"\n",
    "\n",
    "        with open(train_record, 'w') as f:\n",
    "            f.write(str(\"cost:\"))\n",
    "            f.write('\\t')\n",
    "            for itr in range(len(self.train_cost_list)):\n",
    "                f.write(str(self.train_cost_list[itr]))\n",
    "                f.write('\\t')\n",
    "            f.write('\\n')\n",
    "\n",
    "        with open(test_record, 'w') as g:\n",
    "            g.write(str(\"cost:\"))\n",
    "            g.write('\\t')\n",
    "            for itr in range(len(self.test_cost_list)):\n",
    "                g.write(str(self.test_cost_list[itr]))\n",
    "                g.write('\\t')\n",
    "            g.write('\\n')\n",
    "\n",
    "            g.write(str(\"RMSE:\"))\n",
    "            for itr in range(len(self.test_rmse_list)):\n",
    "                g.write(str(self.test_rmse_list[itr]))\n",
    "                g.write('\\t')\n",
    "            g.write('\\n')\n",
    "\n",
    "        with open(basic_info, 'w') as h:\n",
    "            h.write(str(self.args))\n",
    "\n",
    "    def l2_norm(self, tensor):\n",
    "        return tf.sqrt(tf.reduce_sum(tf.square(tensor)))\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### tf.set_random_seed(args.random_seed)\n",
    "np.random.seed(args.random_seed)\n",
    "\n",
    "path = \"data/intersect-20m\"\n",
    "\n",
    "num_users = 138493;  \n",
    "num_items = 15440; \n",
    "# num_total_ratings = 14094614;\n",
    "train_ratio = 0.9\n",
    "\n",
    "# Limit GPU usage\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1409462/1409462 [02:47<00:00, 8419.26it/s]\n"
     ]
    }
   ],
   "source": [
    "R, mask_R, train_R, train_mask_R, eval_R, eval_mask_R, n_train_R, n_eval_R, train_users_idx, train_items_idx, eval_users_idx, eval_items_idx = read_rating(path, train_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train_R = len(R.nonzero()[0]) - n_eval_R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "separator = int(R.shape[0] * 0.9)\n",
    "\n",
    "test_R = R[separator:]\n",
    "test_mask_R = mask_R[separator:]\n",
    "test_train_R = train_R[separator:]\n",
    "test_train_mask_R = train_mask_R[separator:]\n",
    "test_eval_R = eval_R[separator:]\n",
    "test_eval_mask_R = eval_mask_R[separator:]\n",
    "\n",
    "R = R[:separator]\n",
    "mask_R = mask_R[:separator]\n",
    "train_R = train_R[:separator]\n",
    "train_mask_R = train_mask_R[:separator]\n",
    "eval_R = eval_R[:separator]\n",
    "eval_mask_R = eval_mask_R[:separator]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test_eval_R = len(test_eval_R.nonzero()[0]) # non zero\n",
    "n_test_train_R = len(test_R.nonzero()[0]) - n_test_eval_R # non zero from R - non zero eval R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"{}/preprocessed_autorec_dataset\".format(path)\n",
    "pickle.dump((R, mask_R, train_R, train_mask_R, eval_R, eval_mask_R, n_train_R, n_eval_R, train_users_idx, train_items_idx, eval_users_idx, eval_items_idx), open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"{}/preprocessed_autorec_dataset_test\".format(path)\n",
    "pickle.dump((test_R, test_mask_R, test_train_R, test_train_mask_R, test_eval_R, test_eval_mask_R, n_test_train_R, n_test_eval_R, train_users_idx, train_items_idx, eval_users_idx, eval_items_idx), open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load preprocessed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"{}/preprocessed_autorec_dataset\".format(path)\n",
    "R, mask_R, train_R, train_mask_R, eval_R, eval_mask_R, n_train_R, n_eval_R, train_users_idx, train_items_idx, eval_users_idx, eval_items_idx = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<124643x15440 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 12702930 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "with tf.Session(config=config) as sess:\n",
    "    AutoRec = AutoRec(sess, args,\n",
    "                      num_users, num_items,\n",
    "                      R, mask_R, train_R, train_mask_R, eval_R, eval_mask_R,\n",
    "                      n_train_R, n_eval_R,\n",
    "                      train_users_idx, train_items_idx,\n",
    "                      eval_users_idx, eval_items_idx)\n",
    "    AutoRec.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
