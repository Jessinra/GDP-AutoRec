{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check training result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_CODE = \"1560518659.450298\"\n",
    "CHOSEN_EPOCH = 0\n",
    "\n",
    "MODEL_PATH = \"../log/{}/models/epoch_{}\".format(TEST_CODE, CHOSEN_EPOCH)\n",
    "LOG_PATH = \"../log/{}/log.txt\".format(TEST_CODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = open(LOG_PATH).readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hidden_neuron : 1000\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_log = log[1::4]\n",
    "test_log = log[3::4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-ed3c71bf60ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Total cost = \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_log\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Total cost = \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_log\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_RMSE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RMSE = \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_log\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-ed3c71bf60ca>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Total cost = \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_log\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Total cost = \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_log\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_RMSE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RMSE = \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_log\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "train_cost = [float(line.split('\\t')[1].replace(\"Total cost = \",\"\")) for line in train_log][:-1]\n",
    "test_cost = [float(line.split('\\t')[1].replace(\"Total cost = \",\"\")) for line in test_log]\n",
    "test_RMSE = [float(line.split('\\t')[2].replace(\"RMSE = \",\"\")) for line in test_log]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_cost' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-4c23b256f1b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mn_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m138493\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mn_train\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_cost\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtest_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mn_test\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_cost\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_cost' is not defined"
     ]
    }
   ],
   "source": [
    "n_train = int(138493 * 0.9)\n",
    "n_test = int(138493 * 0.1)\n",
    "\n",
    "train_cost = [x/n_train for x in train_cost]\n",
    "test_cost = [x/n_test for x in test_cost]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,10))\n",
    "plt.plot(range(0, len(train_cost)), train_cost, label=\"train\")\n",
    "plt.plot(range(0, len(train_cost)), test_cost, label=\"test\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.hidden_neuron = 1000\n",
    "        self.lambda_value = 1.0\n",
    "        \n",
    "        self.train_epoch = 200\n",
    "        self.batch_size = 2048\n",
    "        \n",
    "        self.optimizer_method = 'Adam' # 'Adam','RMSProp'\n",
    "        self.grad_clip = False\n",
    "        self.base_lr = 0.003\n",
    "        self.decay_epoch_step = 50\n",
    "        \n",
    "        self.random_seed = 1000\n",
    "        self.display_step = 1\n",
    "        self.save_step = 5\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"hidden_neuron : {}\\nlambda_value : {}\\ntrain_epoch : {}\\nbatch_size : {}\\noptimizer_method : {}\\ngrad_clip : {}\\nbase_lr : {}\\ndecay_epoch_step : {}\\nrandom_seed : {}\\ndisplay_step : {}\\n\".format(\n",
    "            self.hidden_neuron,\n",
    "            self.lambda_value,\n",
    "            self.train_epoch,\n",
    "            self.batch_size,\n",
    "            self.optimizer_method,\n",
    "            self.grad_clip,\n",
    "            self.base_lr,\n",
    "            self.decay_epoch_step,\n",
    "            self.random_seed,\n",
    "            self.display_step,\n",
    "            self.save_step)\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "class Logger:\n",
    "\n",
    "    def set_default_filename(self, filename):\n",
    "        self.default_filename = filename\n",
    "\n",
    "    def create_session_folder(self, path):\n",
    "        try:  \n",
    "            os.makedirs(path)\n",
    "        except OSError:  \n",
    "            print (\"Creation of the directory %s failed\" % path)\n",
    "        else:  \n",
    "            print (\"     ===> Successfully created the directory %s \\n\" % path)\n",
    "\n",
    "    def log(self, text):\n",
    "        with open(self.default_filename, 'a') as f:\n",
    "            f.writelines(text)\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "    def save_model(self, model, filename):\n",
    "        pickle.dump(model, open(filename, 'wb'))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "from datetime import datetime\n",
    "from scipy.sparse import lil_matrix, csr_matrix, vstack\n",
    "from tqdm import tqdm\n",
    "\n",
    "class AutoRec():\n",
    "    def __init__(self, sess, args,\n",
    "                 num_users, num_items,\n",
    "                 R, mask_R, train_R, train_mask_R, test_R, test_mask_R, num_train_ratings, num_test_ratings,\n",
    "                 user_train_set, item_train_set, user_test_set, item_test_set):\n",
    "\n",
    "        self.sess = sess\n",
    "        self.args = args\n",
    "\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "\n",
    "        self.R = R\n",
    "        self.mask_R = mask_R\n",
    "        self.train_R = train_R\n",
    "        self.train_mask_R = train_mask_R\n",
    "        self.test_r = test_R\n",
    "        self.test_mask_R = test_mask_R\n",
    "        self.num_train_ratings = num_train_ratings\n",
    "        self.num_test_ratings = num_test_ratings\n",
    "\n",
    "        self.user_train_set = user_train_set\n",
    "        self.item_train_set = item_train_set\n",
    "        self.user_test_set = user_test_set\n",
    "        self.item_test_set = item_test_set\n",
    "\n",
    "        self.hidden_neuron = args.hidden_neuron\n",
    "        self.train_epoch = args.train_epoch\n",
    "        self.batch_size = args.batch_size\n",
    "        self.num_batch = int(\n",
    "            math.ceil(self.num_users / float(self.batch_size)))\n",
    "\n",
    "        self.base_lr = args.base_lr\n",
    "        self.optimizer_method = args.optimizer_method\n",
    "        self.display_step = args.display_step\n",
    "        self.random_seed = args.random_seed\n",
    "\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        self.decay_epoch_step = args.decay_epoch_step\n",
    "        self.decay_step = self.decay_epoch_step * self.num_batch\n",
    "        self.lr = tf.train.exponential_decay(self.base_lr, self.global_step,\n",
    "                                             self.decay_step, 0.96, staircase=True)\n",
    "        self.lambda_value = args.lambda_value\n",
    "\n",
    "        self.train_cost_list = []\n",
    "        self.test_cost_list = []\n",
    "        self.test_rmse_list = []\n",
    "\n",
    "        self.grad_clip = args.grad_clip\n",
    "        \n",
    "        self.timestamp = str(datetime.timestamp(datetime.now()))\n",
    "        self.logger = Logger()\n",
    "        self.session_log_path = \"../log/{}/\".format(self.timestamp)\n",
    "        self.logger.create_session_folder(self.session_log_path)\n",
    "        self.logger.set_default_filename(self.session_log_path + \"log.txt\")\n",
    "\n",
    "    def run(self):\n",
    "        \n",
    "        # Log parameters\n",
    "        self.logger.log(str(self.args))\n",
    "        self.prepare_model()\n",
    "        \n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess.run(init)\n",
    "        \n",
    "        for epoch_itr in (range(self.train_epoch)):\n",
    "            \n",
    "            self.train_model(epoch_itr)\n",
    "            self.test_model(epoch_itr)\n",
    "            \n",
    "            # Save the variables to disk.\n",
    "            if epoch_iter % self.save_step == 0:\n",
    "                self.saver.save(self.sess, self.session_log_path + \"models/epoch_{}\".format(epoch_itr))\n",
    "            \n",
    "        self.make_records()\n",
    "\n",
    "\n",
    "    def prepare_model(self):\n",
    "        self.input_R = tf.placeholder(dtype=tf.float32, shape=[\n",
    "                                      None, self.num_items], name=\"input_R\")\n",
    "        self.input_mask_R = tf.placeholder(\n",
    "            dtype=tf.float32, shape=[None, self.num_items], name=\"input_mask_R\")\n",
    "\n",
    "        V = tf.get_variable(name=\"V\", initializer=tf.truncated_normal(shape=[self.num_items, self.hidden_neuron],\n",
    "                                                                      mean=0, stddev=0.03), dtype=tf.float32)\n",
    "        W = tf.get_variable(name=\"W\", initializer=tf.truncated_normal(shape=[self.hidden_neuron, self.num_items],\n",
    "                                                                      mean=0, stddev=0.03), dtype=tf.float32)\n",
    "        mu = tf.get_variable(name=\"mu\", initializer=tf.zeros(\n",
    "            shape=self.hidden_neuron), dtype=tf.float32)\n",
    "        b = tf.get_variable(name=\"b\", initializer=tf.zeros(\n",
    "            shape=self.num_items), dtype=tf.float32)\n",
    "\n",
    "        pre_Encoder = tf.matmul(self.input_R, V) + mu\n",
    "        self.Encoder = tf.nn.sigmoid(pre_Encoder)\n",
    "        \n",
    "        pre_Decoder = tf.matmul(self.Encoder, W) + b\n",
    "        self.decoder = tf.identity(pre_Decoder)\n",
    "\n",
    "        pre_rec_cost = tf.multiply(\n",
    "            (self.input_R - self.decoder), self.input_mask_R)\n",
    "        rec_cost = tf.square(self.l2_norm(pre_rec_cost))\n",
    "        pre_reg_cost = tf.square(self.l2_norm(W)) + tf.square(self.l2_norm(V))\n",
    "        reg_cost = self.lambda_value * 0.5 * pre_reg_cost\n",
    "\n",
    "        self.cost = rec_cost + reg_cost\n",
    "\n",
    "        if self.optimizer_method == \"Adam\":\n",
    "            optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "        elif self.optimizer_method == \"RMSProp\":\n",
    "            optimizer = tf.train.RMSPropOptimizer(self.lr)\n",
    "        else:\n",
    "            raise ValueError(\"Optimizer Key ERROR\")\n",
    "\n",
    "        if self.grad_clip:\n",
    "            gvs = optimizer.compute_gradients(self.cost)\n",
    "            capped_gvs = [(tf.clip_by_value(grad, -5., 5.), var)\n",
    "                          for grad, var in gvs]\n",
    "            self.optimizer = optimizer.apply_gradients(\n",
    "                capped_gvs, global_step=self.global_step)\n",
    "        else:\n",
    "            self.optimizer = optimizer.minimize(\n",
    "                self.cost, global_step=self.global_step)\n",
    "            \n",
    "        self.saver = tf.train.Saver(max_to_keep=None)\n",
    "\n",
    "    def train_model(self, itr):\n",
    "        start_time = time.time()\n",
    "        random_perm_doc_idx = np.random.permutation(self.num_users)\n",
    "\n",
    "        batch_cost = 0\n",
    "        for i in tqdm(range(self.num_batch)):\n",
    "\n",
    "            if i >= self.num_batch - 1:\n",
    "                batch_set_idx = random_perm_doc_idx[i * self.batch_size:]\n",
    "            else:\n",
    "                batch_set_idx = random_perm_doc_idx[i * self.batch_size: (i + 1) * self.batch_size]\n",
    "\n",
    "            _, cost = self.sess.run(\n",
    "                [self.optimizer, self.cost],\n",
    "                feed_dict={self.input_R: self.train_R[batch_set_idx, :].todense(),\n",
    "                           self.input_mask_R: self.train_mask_R[batch_set_idx, :].todense()})\n",
    "\n",
    "            batch_cost = batch_cost + cost\n",
    "\n",
    "        self.train_cost_list.append(batch_cost)\n",
    "\n",
    "        if (itr + 1) % self.display_step == 0:\n",
    "            self.logger.log(\n",
    "                \"Training Epoch {}\\tTotal cost = {:.2f}\\tElapsed time : {} sec\".format(\n",
    "                    itr, batch_cost, (time.time() - start_time)))\n",
    "            \n",
    "            print(\n",
    "                \"===== Training =====\\n\"\n",
    "                \"Epoch {} \\t Total cost = {:.2f}\\n\"\n",
    "                \"Elapsed time : {} sec\\n\".format(\n",
    "                    itr, batch_cost, (time.time() - start_time)))\n",
    "\n",
    "    def test_model(self, itr):\n",
    "        start_time = time.time()\n",
    "\n",
    "        batch_cost = 0\n",
    "        numerator = 0\n",
    "        \n",
    "        for i in tqdm(range(self.num_batch)):\n",
    "\n",
    "            # Batching idx\n",
    "            batch_start_idx = i * self.batch_size\n",
    "            if i >= self.num_batch - 1:\n",
    "                batch_stop_idx = batch_start_idx + \\\n",
    "                                 (self.num_users - 1) % self.batch_size + 1\n",
    "            else:\n",
    "                batch_stop_idx = (i + 1) * self.batch_size\n",
    "\n",
    "            cost, decoder = self.sess.run(\n",
    "                [self.cost, self.decoder],\n",
    "                feed_dict={self.input_R: self.test_r[batch_start_idx:batch_stop_idx].todense(),\n",
    "                           self.input_mask_R: self.test_mask_R[batch_start_idx:batch_stop_idx].todense()})\n",
    "            \n",
    "            batch_cost += cost\n",
    "\n",
    "            # Make prediction if need to show\n",
    "            if (itr + 1) % self.display_step == 0:\n",
    "\n",
    "                batch_predict_r = csr_matrix(decoder.clip(min=0.2, max=1))\n",
    "\n",
    "                # Some statistic\n",
    "                predicted_rating_delta = batch_predict_r - self.test_r[batch_start_idx:batch_stop_idx]\n",
    "                pre_numerator = self.test_mask_R[batch_start_idx:batch_stop_idx].multiply(predicted_rating_delta)\n",
    "                numerator += np.sum(pre_numerator.data ** 2)\n",
    "\n",
    "        self.test_cost_list.append(batch_cost)\n",
    "\n",
    "        # Make prediction if need to show\n",
    "        if (itr + 1) % self.display_step == 0:\n",
    "\n",
    "            denominator = self.num_test_ratings\n",
    "            RMSE = np.sqrt(numerator / float(denominator))\n",
    "            self.test_rmse_list.append(RMSE)\n",
    "\n",
    "            self.logger.log(\n",
    "                \"Testing Epoch {}\\tTotal cost = {:.2f}\\tRMSE = {:.5f}\\tElapsed time : {} sec\".format(\n",
    "                    itr, batch_cost, RMSE, (time.time() - start_time)))\n",
    "\n",
    "            print(\n",
    "                \"===== Testing =====\\n\"\n",
    "                \"Epoch {} \\t Total cost = {:.2f}\\n\"\n",
    "                \"RMSE = {:.5f} \\t Elapsed time : {} sec\\n\".format(\n",
    "                    itr, batch_cost, RMSE, (time.time() - start_time)))\n",
    "\n",
    "    def make_records(self):\n",
    "\n",
    "        basic_info = self.session_log_path + \"basic_info.txt\"\n",
    "        train_record = self.session_log_path + \"train_record.txt\"\n",
    "        test_record = self.session_log_path + \"test_record.txt\"\n",
    "\n",
    "        with open(train_record, 'w') as f:\n",
    "            f.write(str(\"cost:\"))\n",
    "            f.write('\\t')\n",
    "            for itr in range(len(self.train_cost_list)):\n",
    "                f.write(str(self.train_cost_list[itr]))\n",
    "                f.write('\\t')\n",
    "            f.write('\\n')\n",
    "\n",
    "        with open(test_record, 'w') as g:\n",
    "            g.write(str(\"cost:\"))\n",
    "            g.write('\\t')\n",
    "            for itr in range(len(self.test_cost_list)):\n",
    "                g.write(str(self.test_cost_list[itr]))\n",
    "                g.write('\\t')\n",
    "            g.write('\\n')\n",
    "\n",
    "            g.write(str(\"RMSE:\"))\n",
    "            for itr in range(len(self.test_rmse_list)):\n",
    "                g.write(str(self.test_rmse_list[itr]))\n",
    "                g.write('\\t')\n",
    "            g.write('\\n')\n",
    "\n",
    "        with open(basic_info, 'w') as h:\n",
    "            h.write(str(self.args))\n",
    "\n",
    "    def l2_norm(self, tensor):\n",
    "        return tf.sqrt(tf.reduce_sum(tf.square(tensor)))\n",
    "\n",
    "    \n",
    "    # ============  CUSTOM FOR EVALUATION  ======================\n",
    "    \n",
    "    def custom_run(self):\n",
    "        \n",
    "        # Log parameters\n",
    "        self.prepare_model()\n",
    "        \n",
    "#         init = tf.global_variables_initializer()\n",
    "#         self.sess.run(init)\n",
    "        \n",
    "\n",
    "    def predict(self, rating, mask_rating):\n",
    "\n",
    "        batch_cost = 0\n",
    "        predict_r = csr_matrix((0, rating.shape[1]))\n",
    "        num_batch = int(math.ceil(rating.shape[0] / float(self.batch_size)))\n",
    "        \n",
    "        for i in tqdm(range(num_batch)):\n",
    "\n",
    "            # Batching idx\n",
    "            batch_start_idx = i * self.batch_size\n",
    "            if i >= self.num_batch - 1:\n",
    "                batch_stop_idx = batch_start_idx + \\\n",
    "                                 (self.num_users - 1) % self.batch_size + 1\n",
    "            else:\n",
    "                batch_stop_idx = (i + 1) * self.batch_size\n",
    "\n",
    "            cost, decoder = self.sess.run(\n",
    "                [self.cost, self.decoder],\n",
    "                feed_dict={self.input_R: rating[batch_start_idx:batch_stop_idx].todense(),\n",
    "                           self.input_mask_R: mask_rating[batch_start_idx:batch_stop_idx].todense()})\n",
    "            \n",
    "            batch_cost += cost\n",
    "            batch_predict_r = csr_matrix(decoder.clip(min=0.2, max=1)) \n",
    "            predict_r = vstack([predict_r, batch_predict_r])\n",
    "            \n",
    "#             print(predict_r.nonzero())\n",
    "#             input()\n",
    "            \n",
    "        return predict_r, batch_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Precision at K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"../data/intersect-20m/preprocessed_autorec_dataset\"\n",
    "R, mask_R, train_R, train_mask_R, test_R, test_mask_R, n_train_R, n_test_R, train_users_idx, train_items_idx, test_users_idx, test_items_idx = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to reduce size \n",
    "\n",
    "rating = test_R[::25]\n",
    "mask_rating = test_mask_R[::25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ======"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random seeding\n",
    "tf.set_random_seed(args.random_seed)\n",
    "np.random.seed(args.random_seed)\n",
    "\n",
    "# Detail about dataset\n",
    "path = \"data/intersect-20m\"\n",
    "num_users = 138493\n",
    "num_items = 15085\n",
    "train_ratio = 0.9\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jessinra/.local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "     ===> Successfully created the directory ../log/1560519144.850421/ \n",
      "\n",
      "WARNING:tensorflow:From /home/jessinra/.local/lib/python3.5/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from ../log/1560518659.450298/models/epoch_0\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "model = AutoRec(sess, args,\n",
    "                  num_users, num_items,\n",
    "                  R, mask_R, train_R, train_mask_R, test_R, test_mask_R,\n",
    "                  n_train_R, n_test_R,\n",
    "                  train_users_idx, train_items_idx,\n",
    "                  test_users_idx, test_items_idx)\n",
    "\n",
    "model.custom_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../log/1560518659.450298/models/epoch_0\n"
     ]
    }
   ],
   "source": [
    "# Add ops to save and restore all the variables.\n",
    "# saver = tf.train.Saver()\n",
    "model.saver = tf.train.import_meta_graph(MODEL_PATH + \".meta\")\n",
    "model.saver.restore(sess, MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:08<00:00,  2.70s/it]\n"
     ]
    }
   ],
   "source": [
    "prediction, cost = model.predict(rating, mask_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "prediction_df = pd.DataFrame(prediction.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   0,    0,    0, ..., 5537, 5537, 5539], dtype=int32),\n",
       " array([ 1049,  4651,  4671, ..., 11316, 14917,  9135], dtype=int32))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating.nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>11315</th>\n",
       "      <th>11316</th>\n",
       "      <th>11317</th>\n",
       "      <th>11318</th>\n",
       "      <th>11319</th>\n",
       "      <th>11320</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5535</th>\n",
       "      <td>0.782782</td>\n",
       "      <td>0.852575</td>\n",
       "      <td>0.757772</td>\n",
       "      <td>0.548371</td>\n",
       "      <td>0.375569</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5536</th>\n",
       "      <td>0.785897</td>\n",
       "      <td>0.859321</td>\n",
       "      <td>0.778157</td>\n",
       "      <td>0.553110</td>\n",
       "      <td>0.380212</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5537</th>\n",
       "      <td>0.772272</td>\n",
       "      <td>0.866146</td>\n",
       "      <td>0.780924</td>\n",
       "      <td>0.551829</td>\n",
       "      <td>0.408091</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5538</th>\n",
       "      <td>0.791989</td>\n",
       "      <td>0.861440</td>\n",
       "      <td>0.786462</td>\n",
       "      <td>0.555019</td>\n",
       "      <td>0.349497</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5539</th>\n",
       "      <td>0.790784</td>\n",
       "      <td>0.859803</td>\n",
       "      <td>0.784278</td>\n",
       "      <td>0.556355</td>\n",
       "      <td>0.353539</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         11315     11316     11317     11318     11319  11320\n",
       "5535  0.782782  0.852575  0.757772  0.548371  0.375569    1.0\n",
       "5536  0.785897  0.859321  0.778157  0.553110  0.380212    1.0\n",
       "5537  0.772272  0.866146  0.780924  0.551829  0.408091    1.0\n",
       "5538  0.791989  0.861440  0.786462  0.555019  0.349497    1.0\n",
       "5539  0.790784  0.859803  0.784278  0.556355  0.353539    1.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_df.loc[5535:5540, 11315:11320]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Somehow, model weight is not loaded for some reason\n",
    "- already changed preprocess and loader, but still no changes\n",
    "\n",
    "model.custom_run() if no -> no cost to run\n",
    "\n",
    "need initializer , but if use initializer , then weight is not imported"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is it not saved ?  or is it not loaded ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
